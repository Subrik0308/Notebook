{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Содержание:\n",
    "- Извлечение данных из веб-страницы. Введение в HTML\n",
    "- Один из возможных алгоритмов скрейпинга\n",
    "- Cлучайный вопрос из базы вопросов ЧГК\n",
    "- Парсим данные из Wikipedia\n",
    "- Работа с API. JSON\n",
    "- Работа с API Headhunter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Извлечение данных из веб-страницы\n",
    "\n",
    "## Введение в HTML\n",
    "\n",
    "**HTML (HyperText Markup Language)** – язык разметки (для тех, кто работал с *RMarkdown*: *Markdown* ‒  тоже язык разметки, только попроще). Поэтому, любая html-страница в интернете – это обычный текст, в котором отмечено, какие части представляют собой заголовок, абзац, таблицу, картинку и так далее. Вообще, правилом хорошего тона считается включать в html-код только содержательные вещи с разметкой, а все оформление и интерактив выносить в отдельные файлы.\n",
    "\n",
    "Обычно, за страницей в интернете кроется примерно следующее. На сервере лежит несколько папок. В одной папке лежат сами html-страницы с размеченным текстом. В другой – хранятся файлы *css* (*Cascading Style Sheets*), в которых заданы параметры оформления разных страниц (цвет заголовков, фона, меню, ширина рамочек вокруг текста и картинок и прочее). В третьей папке находятся файлы с кодом на *JavaScript*, которые отвечают за всевозможный интерактив: будь то всплывающий перевод слова при наведении курсора, увеличение картинки при просмотре, подсветка формы для заполнения, если формат даты неверный, и так далее. JavaScript ‒ тоже отдельный язык программирования (несмотря на название, с Java не связан), на котором пишется код, соответствующий интерактиву.\n",
    "\n",
    "Все эти файлы связаны между собой: к каждому html-файлу присоединены с помощью ссылок css-файл с оформлением и код javascript. Иногда, к сожалению, html-страницы создаются не по стандартам, и в файле с текстом встречаются огромные куски, отвечающие за оформление, что значительно затрудняет работу.\n",
    "\n",
    "Посмотрим что такое веб-скрейпинг и парсинг html-файлов. Обычно два этих процесса связаны, просто парсинг – более узкое понятие. Веб-скрейпинг (*web scraping*) – это вообще выгрузка данных с веб-страниц, а парсинг (*parsing*) – это выбор текста из файла с разметкой, например, из html или xml. Но прежде, чем разбирать работу с html-файлами, давайте познакомимся поближе с их структурой. Для этого зайдем на сайт [w3schools.com](https://www.w3schools.com/Html/) и откроем [раздел](https://www.w3schools.com/Html/tryit.asp?filename=tryhtml_default) *Try it yourself*.\n",
    "\n",
    "Это удобный учебный инструмент для создания html-файлов (и да, вообще на сайте много документации и тьюториалов по html, css и веб-дизайну вообще). Давайте создадим свою страничку.\n",
    "\n",
    "Для разметки используются тэги – служебные слова в треугольных скобках `<>`. Тэги бывают разными: открывающие и закрывающие. Закрывающие тэги начинаются с прямого слэша (`/`).\n",
    "\n",
    "Вся страница заключается в тэг `<html></html>`. В начале обычно указывается какая-то мета-информация: язык страницы, кодировка, метки, название и прочее (в примере этого нет). \"Тело\" документа, собственно страница, которая отображается при просмотре, заключается в тэг `<body></body>`.\n",
    "\n",
    "Для заголовков используются тэги с `h` (`h` – от *header*). Тэги `<h1></h1>`  ‒ для заголовков первого уровня, тэг `<h2></h2>` – для заголовков уровня, и так далее. В тэги `<p></p>` заключаются абзацы (`p` – от *paragraph*). Для начала внесем изменения в файл ‒ создадим страничку о себе. Можете скопировать код ниже в поле на странице *Try it Yourself* и нажать кнопку *Run*.\n",
    "\n",
    "Ниже будет код *html* поэтому запустить в Jupyter Notebook его не получится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h1>Моя первая html-страница</h1>\n",
    "<h2>О себе</h2>\n",
    "<p>Я учусь создавать html-страницы.</p>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](html2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим небольшую таблицу. Знания об устройстве таблиц понадобятся, потому что чаще всего при парсинге приходится «доставать» данные из таблиц на html-страницах.\n",
    "\n",
    "Вся таблица заключается в тэги `<table></table>`. Далее таблица заполняется по строкам. Строка таблицы заключается в тэги `<tr></tr>` (от *table row*). Ячейки таблицы тоже имеют свои тэги. Если ячейка обычная, то есть не является названием столбца или строки, она окружена тэгом `<td></td>` (от *table data*), если является ‒ то `<th></th>` (от *table header*).\n",
    "\n",
    "Создадим свою таблицу:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<table>\n",
    "<tr>\n",
    "<th>Дата</th>\n",
    "<th>Имя</th>\n",
    "<th>Возраст</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Иванов</td>\n",
    "<td>Иван</td>\n",
    "<td>28</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](html3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можем добавить явные границы ячеек – добавить атрибут *border* и отрегулировать ширину."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<table border = \"1\">\n",
    "<tr>\n",
    "<th>Фамилия</th>\n",
    "<th>Имя</th>\n",
    "<th>Возраст</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Иванов</td>\n",
    "<td>Иван</td>\n",
    "<td>28</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](html4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим заголовок."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h1>Моя первая html-страница</h1>\n",
    "<h2>О себе</h2>\n",
    "<p>Я учусь создавать html-страницы.</p>\n",
    "\n",
    "<table border = \"1\">\n",
    "<caption>Информация</caption>\n",
    "<tr>\n",
    "<th>Фамилия</th>\n",
    "<th>Имя</th>\n",
    "<th>Возраст</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Иванов</td>\n",
    "<td>Иван</td>\n",
    "<td>28</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Один из возможных алгоритмов скрейпинга\n",
    "\n",
    "Для этого используется библиотека requests.\n",
    "\n",
    "- указать в коде адрес интересующего сайта: откуда вы хотите скачать данные?\n",
    "- сохранить веб-страницу (html-код страницы)\n",
    "- выбрать данные, которые нужно собрать (используется BeautifulSoup)\n",
    "- записать данные в csv-файл.\n",
    "\n",
    "Если нужно соскрейпить несколько страниц – повторяем процесс для каждой из них.\n",
    "\n",
    "**Наша задача:** выгрузить недавние новости в датафрейм `pandas`, чтобы потом сохранить все в csv-файл.\n",
    "\n",
    "Сначала сгрузим весь html-код страницы и сохраним его в отдельную переменную. Для этого нам понадобится библиотека `requests`. Документация: https://requests.readthedocs.io/en/latest/\n",
    "\n",
    "Импортируем её:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним ссылку на главную страницу сайта в переменную `url` для удобства и выгрузим страницу. (Разумеется, это будет работать при подключении к интернету. Если соединение будет отключено, Python выдаст `NewConnectionError`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://nplus1.ru/'  # Сохраняем\n",
    "page = requests.get(url)  # Загружаем страницу по ссылке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы просто посмотрим на объект, мы ничего особенного не увидим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page  # response 200 – страница загружена"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BeautifulSoup** – python-библиотека для синтаксического разбора файлов HTML/XML. В веб-разработке «суп из тегов» (tag soup) – это слово для синтаксически или структурно некорректного HTML, написанного для веб-страницы.\n",
    "\n",
    "Документация: https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "\n",
    "Импортируем функцию `BeautifulSoup` из библиотеки `bs4` (от *beautifulsoup4*) и заберём со страницы `page` код html в виде текста.\n",
    "\n",
    "Сохраним в переменную 'soup' весь HTML-код страницы. HTML-код – это \"дерево тегов\", формирующее контент страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# Название – отсылка к песне про суп из Алисы в стране чудес https://aliceinwonderland.fandom.com/wiki/Turtle_Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text, 'html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если выведем `soup` на экран, мы увидим то же самое, что в режиме разработчика или в режиме происмотра исходного кода (`view-source` через *Ctrl+U* в Google Chrome)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для просмотра выглядит не очень удобно.  Воспользуемся методом `.prettify()` в сочетании с функцией `print()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В такой выдаче ориентироваться гораздо удобнее (но при желании, то же можно увидеть в браузере, на большом экране).\n",
    "\n",
    "Чтобы загрузить все новости с главной страницы сайта, нужно собрать все ссылки на страницы с этими новостями. Ссылки в html-файле всегда заключены в тэг `<a></a>` и имеют атрибут `href`.\n",
    "\n",
    "**Функция `soup.find('a')`** найдёт первый в дереве тег \\<a>.\n",
    "Если нам нужно найти не только первый элемент, а все элементы по определенному признаку, следует использовать функцию **`soup.find_all('a')`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')[0].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(soup.find_all('a'))  # Всего элементов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')[0].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылок много. Нужны только новости – ссылки, которые начинаются со слова `/news`. Добавим условие: будем выбирать только те ссылки, в которых есть `/news`. Создадим пустой список `urls` и будем добавлять в него только ссылки, которые удовлетворяют этому условию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "    if '/news' in link.get('href'):\n",
    "        if 'https://' in link.get('href'):\n",
    "            urls.append(link.get('href'))\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь наша задача сводится к следующему: изучить одну страницу с новостью, научиться из нее вытаскивать текст и всю необходимую информацию, а потом применить весь набор действий к каждой ссылке из `full_urls` в цикле. Посмотрим на новость с индексом 0. Так как новости обновляются спустя какое-то время может быть другая страница."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url0 = urls[0]\n",
    "\n",
    "page0 = requests.get(url0)\n",
    "print(page0)\n",
    "\n",
    "soup0 = BeautifulSoup(page0.text, 'html')\n",
    "print(soup0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В коде каждой страницы с новостью есть часть с мета-информацией: датой, именем автора и проч. Такая информация окружена тэгом `<meta></meta>`. Посмотрим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find_all('meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find_all('meta')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find_all('meta')[1].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find_all('meta', {'name': 'viewport'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find('meta', {'name': 'viewport'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём название статьи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find('meta', {'property': 'og:title'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find('meta', {'property': 'og:title'}).get('content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find('meta', {'property': 'og:title'}).get('content').replace('\\xa0', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём дату публикации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find_all('meta', {'itemprop': 'datePublished'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find_all('meta', {'itemprop': 'datePublished'})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find_all('meta', {'itemprop': 'datePublished'})[0].get('content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдём автора и сохраним в переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find_all('meta', {'name' : 'author'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь выберем единственный элемент полученного списка (с индексом 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup0.find_all('meta', {'name' : 'author'})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author = soup0.find('meta', {'name' : 'author'}).get('content')\n",
    "author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Аналогичным образом извлечём дату, заголовок и описание."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = soup0.find_all('meta', {'itemprop' : 'datePublished'})[0].get('content')\n",
    "title = soup0.find_all('meta', {'property' : 'og:title'})[0].get('content').replace('\\xa0', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем готовую функцию для всех проделанных действий и применим её в цикле для всех ссылок в списке `full_urls`. Аргументом функции будет ссылка на новость, а возвращать она будет текст новости и всю необходимую информацию (дата, автор, сложность и проч.). Скопируем все строки кода выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNews(url0):\n",
    "    \"\"\"\n",
    "    Возвращает кортеж с url0, date, author, title.\n",
    "    Параметры:\n",
    "\n",
    "    url0 – ссылка на новость (строка).\n",
    "    \"\"\"\n",
    "    page0 = requests.get(url0)\n",
    "    soup0 = BeautifulSoup(page0.text, 'html')\n",
    "\n",
    "    author = soup0.find_all('meta', {'name' : 'author'})[0].get('content')\n",
    "    date = soup0.find_all('meta', {'itemprop' : 'datePublished'})[0].get('content')\n",
    "    title = soup0.find_all('meta', {'property' : 'og:title'})[0].get('content').replace('\\xa0', ' ')\n",
    "\n",
    "    return url0, date, author, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(GetNews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось применить её в цикле.\n",
    "\n",
    "Импортируем функцию `sleep` для задержки, чтобы на каждой итерации цикла, прежде чем перейти к следующей новости, Python ждал несколько секунд. Во-первых, это нужно, чтобы сайт «не понял», чтобы мы его парсим, да еще автоматически. Во-вторых, с небольшой задержкой всегда есть гарантия, что страница прогрузится (сейчас это пока не очень важно, но особенно актуально будет, когда будем обсуждать встраивание в браузер с Selenium)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []  # Это будет список из кортежей, в которых будут храниться данные по каждой новости\n",
    "\n",
    "for link in urls[1:]:\n",
    "    print(link)\n",
    "    res = GetNews(link)\n",
    "    news.append(res)\n",
    "\n",
    "    sleep(1)  # Задержка в 1 секунду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetNews(url0):\n",
    "    \"\"\"\n",
    "    Возвращает кортеж с url0, date, author, title.\n",
    "    Параметры:\n",
    "\n",
    "    url0 – ссылка на новость (строка).\n",
    "    \"\"\"\n",
    "    page0 = requests.get(url0)\n",
    "    soup0 = BeautifulSoup(page0.text, 'html')\n",
    "\n",
    "    try:\n",
    "        author = soup0.find_all('meta', {'name' : 'author'})[0].get('content')\n",
    "    except:\n",
    "        author = None\n",
    "    date = soup0.find_all('meta', {'itemprop' : 'datePublished'})[0].get('content')\n",
    "    title = soup0.find_all('meta', {'property' : 'og:title'})[0].get('content').replace('\\xa0', ' ')\n",
    "\n",
    "    return url0, date, author, title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = []  # Это будет список из кортежей, в которых будут храниться данные по каждой новости\n",
    "l = len(urls)  # Количество ссылок\n",
    "\n",
    "for link in urls:\n",
    "    print(f\"Осталось {l}\")\n",
    "    print(link)\n",
    "    res = GetNews(link)\n",
    "    news.append(res)\n",
    "    l -= 1\n",
    "\n",
    "    sleep(1)  # Задержка в 1 секунду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так теперь выглядит первый элемент списка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем `pandas` и создадим датафрейм из списка кортежей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переименуем столбцы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['link', 'date', 'author', 'title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё! Сохраняем датафрейм в файл. Для разнообразия сохраним в Excel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('nplus-news.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](html5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cлучайный вопрос из [базы вопросов ЧГК](https://db.chgk.info/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://db.chgk.info/tour/ovsch20.5_u\"  # Возьмем конкретный турнир\n",
    "\n",
    "page = requests.get(url)\n",
    "page.encoding = 'utf-8'\n",
    "soup = BeautifulSoup(page.text)\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"div\", {\"class\": \"question\"})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = soup.find_all(\"div\", {\"class\": \"question\"})\n",
    "num_questions = len(questions)\n",
    "num_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0].find_all('p')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0].find_all('p')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions[0].find_all('p')[0].text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = randint(0, num_questions - 1)\n",
    "\n",
    "q = questions[num].find_all('p')[0]\n",
    "print(q.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим задержку между вопросом и ответом\n",
    "\n",
    "from random import randint\n",
    "import time\n",
    "\n",
    "num = randint(0, num_questions - 1)\n",
    "\n",
    "q = questions[num].find_all('p')[0]\n",
    "a = questions[num].find_all('p')[1]\n",
    "\n",
    "# attributes = []\n",
    "# for i in range(2,7):\n",
    "#     try:\n",
    "#         attributes.append(questions[num].find_all('p')[i].text)\n",
    "#     except:\n",
    "#         pass\n",
    "\n",
    "print(q.text)\n",
    "\n",
    "time.sleep(10)  # 10 секунд\n",
    "\n",
    "print(a.text)\n",
    "\n",
    "# for a in attributes:\n",
    "#     if a.strip().startswith('Зачёт'):\n",
    "#         print(a)\n",
    "#     if a.strip().startswith('Комментарий'):\n",
    "#         print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Парсим данные из Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://ru.wikipedia.org/wiki/Python')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(page.text)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[:6][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_clean = [par.text for par in p[:6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_clean[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikipedia.txt', 'w', encoding = 'utf-8') as infile:\n",
    "    infile.writelines(p_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get('https://ru.wikipedia.org/wiki/Python')\n",
    "soup = BeautifulSoup(page.text)\n",
    "p = soup.find_all('p')\n",
    "p_clean = [par.text for par in p[:6]]\n",
    "with open('wikipedia.txt', 'w', encoding = 'utf-8') as infile:\n",
    "    infile.writelines(p_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Работа с API\n",
    "\n",
    "До этого сбор данных производился \"вручную\", обращаясь к html страницам, размеченным для отображения в браузере. Но данные также можно собирать и через API –  – application program interface. Обычный интерфейс – это способ взаимодействия человека с программой, а API – одной программы с другой. Например, вашего скрипта на Python с удалённым веб-сервером. \n",
    "\n",
    "Для хранения веб-страниц, которые читают люди, используется язык HTML. Для хранения произвольных структурированных данных, которыми обмениваются между собой программы, используются другие языки – в частности, язык XML, похожий на HTML. Вернее было бы сказать, что XML это метаязык, то есть способ описания языков. В отличие от HTML, набор тегов в XML-документе может быть произвольным (и определяется разработчиком конкретного диалекта XML). На текущий момент XML считается устаревшим, и чаще всего применяется JSON.\n",
    "\n",
    "## JSON\n",
    "\n",
    "Один из популярных формат, в котором сайт может отдать вам данные – json. JSON расшифровывается как JavaScript Object Notation и изначально возник как подмножество языка JavaScript (пусть вас не вводит в заблуждение название, этот язык ничего не имеет общего с Java), используемое для описания объектов, но впоследствии стал использоваться и в других языках программирования, включая Python. Различные API могут поддерживать либо XML, либо JSON, либо и то, и другое.\n",
    "\n",
    "JSON очень похож на описание объекта в Python и смысл квадратных и фигурных скобок такой же. Правда, есть и отличия: например, в Python одинарные и двойные кавычки ничем не отличаются, а в JSON можно использовать только двойные. Мы видим, что полученный нами JSON представляет собой словарь, значения которого – строки или числа, а также списки или словари, значения которых в свою очередь также могут быть строками, числами, списками, словарями и т.д. То есть получается такая довольно сложная структура данных.\n",
    "\n",
    "В данный момент тот факт, что перед нами сложная структура данных, видим только мы – с точки зрения Python, j.text это просто такая строка. Однако в модуле requests есть метод, позволяющий сразу выдать питоновский объект (словарь или список), если результат запроса возвращён в формате JSON. Так что нам не придётся использовать никакие дополнительные библиотеки.\n",
    "\n",
    "Рассмотрим сайт https://kudago.com/msk/ с открытм API. Документацию по API можно найти по ссылке https://docs.kudago.com/api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для запроса\n",
    "\n",
    "def get_requests(subject, params):\n",
    "    url = \"https://kudago.com/\"\n",
    "    path = \"/public-api\"\n",
    "    version = \"/v1.4\"\n",
    "    request = url + path + version + subject\n",
    "    print(request)\n",
    "    return requests.get(request, params = params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример КАТЕГОРИИ СОБЫТИЙ:\n",
    "# https://kudago.com/public-api/v1.2/event-categories/?lang=ru&order_by=id\n",
    "\n",
    "event_categories = \"/event-categories\"\n",
    "params = {\"lang\" : \"ru\", \"order_by\": \"id\"}\n",
    "\n",
    "response = get_requests(event_categories, params = params)\n",
    "print(response.url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj = response.json()\n",
    "json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример КАТЕГОРИИ МЕСТ:\n",
    "# https://kudago.com/public-api/v1.2/place-categories/?lang=ru&order_by=id\n",
    "\n",
    "place = \"/place-categories\"\n",
    "params = {\"lang\" : \"ru\", \"order_by\": \"id\"}\n",
    "\n",
    "response = get_requests(place, params = params)\n",
    "print(response.url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj = response.json()\n",
    "json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пример ГОРОДА\n",
    "# https://kudago.com/public-api/v1.4/locations/?lang=ru&fields=slug%2Ctimezone%2Ccoords\n",
    "\n",
    "place = \"/locations\"\n",
    "params = {\"lang\" : \"ru\", \"fields\": \"name, slug, timezone, coords\"}\n",
    "\n",
    "response = get_requests(place, params = params)\n",
    "print(response.url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj_locations = response.json()\n",
    "json_obj_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# На основе полученных данных обратимся к городам\n",
    "\n",
    "place = \"/locations/\"\n",
    "slag = json_obj_locations[0][\"slug\"]\n",
    "params = {\"lang\" : \"ru\"}\n",
    "\n",
    "response = get_requests(place + slag, params = params)\n",
    "print(response.url)\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj = response.json()\n",
    "json_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с API Headhunter\n",
    "\n",
    "Проведем анализ вакансий Ростелекома на HeadHunter. Документация API hh: https://api.hh.ru/openapi/redoc\n",
    "\n",
    "https://api.hh.ru/vacancies\n",
    "\n",
    "https://api.hh.ru/vacancies?page=100\n",
    "\n",
    "https://api.hh.ru/vacancies?text=%D1%80%D0%BE%D1%81%D1%82%D0%B5%D0%BB%D0%B5%D0%BA%D0%BE%D0%BC&search_field=company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from tqdm import tqdm  # Время\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запросы к API всегда выстраиваются в таком формате: \n",
    "\n",
    "URL API + нужный раздел + API-ключ (по требованию) + параметры\n",
    "\n",
    "https://api.hh.ru/vacancies?employer_id=2748&only_with_salary=true&per_page=100&period=7\n",
    "\n",
    "Что в этом запросе?\n",
    "\n",
    "`https://api.hh.ru/vacancies` – запрос к БД с вакансиями\n",
    "\n",
    "`?employer_id=2748` – прицепляем с помощью **?** первый параметр `employer_id` с ID [Ростелекома](https://hh.ru/employer/2748)\n",
    "\n",
    "`&only_with_salary=true` – все следующие параметры прицепляем через **&**. Ищем вакансии только с указанной з/п\n",
    "\n",
    "`&per_page=100` – используем пагинацию (на каждой странице по 100 вакансий). Ограничения по глубине запросов – 2000 вакансий\n",
    "\n",
    "`&period=7` – смотрим только на вакансии, выложенные в течение недели.\n",
    "\n",
    "Для начала посмотрим, сколько у нас всего вакансий по таким параметрам. Для этого делаем запрос через `requests.get()` к ссылке выше. С помощью библиотеки json загружаем данные (фактически преобразовываем их в словарь) и смотрим на ключ `'found'` (количество найденных вакансий)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = requests.get('https://api.hh.ru/vacancies?employer_id=2748&only_with_salary=true&per_page=100&period=7')\n",
    "print(json.loads(test.text).keys())\n",
    "json.loads(test.text)['found']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласно ограничениям в документации мы не сможем выгрузить более 2000 вакансий. Поэтому остановимся на этой цифре. Будем перебирать страницы (на каждой по 100 вакансий) с помощью параметра `page`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancies = []\n",
    "\n",
    "for i in tqdm(range(0, 20)):\n",
    "    vac = requests.get(f\"https://api.hh.ru/vacancies?employer_id=2748&only_with_salary=true&per_page=100&period=7&page={i}\")\n",
    "    vacancies.extend(json.loads(vac.text)['items'])  # extend – объединение\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancies[0]  # Первая выгруженная вакансия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacancies[3]['snippet']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Анализировать все в формате json неудобно, поэтому соберем нужные данные в таблицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = []\n",
    "\n",
    "for v in vacancies:\n",
    "    table.append((v['name'], v['salary']['from'], v['salary']['to'], v['area']['name'], v['url']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table, columns = ['vacancy', 'sal_from', 'sal_to', 'area', 'url'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сравнить, какие специалисты и навыки нужны в Абакане и Москве, отфильтруем таблицу и сделаем новые запросы к API уже по конкретным url вакансий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msc_abk_rtc = df[(df['area'] == 'Москва') | (df['area'] == 'Абакан')]['url'].tolist()\n",
    "len(msc_abk_rtc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вытащим из каждой вакансии описание и ключевые навыки (то, что мы не можем достать со страницы общего поиска)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_test = requests.get(msc_abk_rtc[0])\n",
    "test0 = json.loads(mk_test.text)\n",
    "test0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0['key_skills']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i['name'] for i in test0['key_skills']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем в одну таблицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = []\n",
    "\n",
    "for l in tqdm(msc_abk_rtc):\n",
    "    v = requests.get(l)\n",
    "    v_loaded = json.loads(v.text)\n",
    "\n",
    "    name = v_loaded['name']\n",
    "    area = v_loaded['area']['name']\n",
    "    descr = v_loaded['description']\n",
    "    skills = [i['name'] for i in v_loaded['key_skills']]\n",
    "    money_from = v_loaded['salary'].get('from', np.nan)\n",
    "    money_to = v_loaded['salary'].get('to', np.nan)\n",
    "    url = v_loaded['alternate_url']\n",
    "\n",
    "    info.append((name, area, descr, skills, money_from, money_to, url))\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk = pd.DataFrame(info, columns = ['name', 'area', 'description', 'key_skills', 'salary_from', 'salary_to', 'url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "sns.boxplot(data = df_mk, y = 'salary_from', x = 'area');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "sns.boxplot(data = df_mk, y = 'salary_to', x = 'area');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk[df_mk['salary_to'] == df_mk['salary_to'].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk[(df_mk['area'] == 'Абакан')].sort_values('salary_from', ascending = False).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk[df_mk['area'] == 'Москва'][['salary_from', 'salary_to']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk[df_mk['area'] == 'Абакан'][['salary_from', 'salary_to']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь проанализируем, какие ключевые навыки нужны для вакансий в Москве и Абакане."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df_mk[df_mk['area'] == 'Абакан']['key_skills'].sum()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(df_mk[df_mk['area'] == 'Москва']['key_skills'].sum()).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почистим описания вакансий от html-разметки. В этом нам помогут [регулярные выражения](https://regex101.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk['description'].str.replace('</?\\w+>', '', regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mk['description'] = df_mk['description'].str.replace('</?\\w+>', '', regex = True)\n",
    "df_mk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
